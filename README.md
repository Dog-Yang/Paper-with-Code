# Content
[[VPR](#VPR)] [[Token fusion](#Token_fusion)] [[Training Free](#Training_Free)] [[Classical segmentation](#Classical_Segmentation)] [[Classical detection](#Model-Classical_detection)] [[Backbone](#Backbone)] [[CLIP](#CLIP)] [[Open vocabulary segmentation](#open_segmentation)] [[Open vocabulary detection](#open_detection)] [[Other](#Other)]
-----------------------------------------------------------------------------------------------
<a name="Training_Free"></a>
# Training Free
[[2022 ECCV](https://arxiv.org/pdf/2112.01071)] [[code](https://github.com/chongzhou96/MaskCLIP)] Maskclip: Extract Free Dense Labels from CLIP    
[[2024 NeurIPS](https://arxiv.org/pdf/2406.01837)] [[code](https://github.com/MaxZanella/transduction-for-vlms)] Transclip: Boosting Vision-Language Models with Transduction    
[[2024 ECCV](https://arxiv.org/pdf/2312.01597)] [[code](https://github.com/wangf3014/SCLIP)] SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference    
[[2024 WACV](https://arxiv.org/pdf/2404.08181)] [[code](https://github.com/sinahmr/NACLIP)] NACLIP: Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation       
[[2024 ECCV](https://arxiv.org/pdf/2408.04883v1)] [[code](https://github.com/mc-lan/ProxyCLIP)] ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation    
[[2024 ECCV](https://arxiv.org/pdf/2312.12359)] [[code](https://github.com/wysoczanska/clip_dinoiser)] CLIP_Dinoiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation    
[[2024 Arxiv](https://arxiv.org/pdf/2411.15869)] [[code](https://github.com/SuleBai/SC-CLIP)] SC-CLIP: Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation    
[[2024 NeurIPS](https://arxiv.org/pdf/2404.08461)] [[code](https://github.com/sprocketlab/otter)] OTTER: Effortless Label Distribution Adaptation of Zero-shot Models    
[[2025 CVPR](https://arxiv.org/pdf/2505.24693)] [[code](https://github.com/jusiro/CLIP-Conformal)] Conformal Prediction for Zero-Shot Models     
[[2025 ICML](https://arxiv.org/abs/2505.04560)] [[code](https://github.com/ghwang-s/abkd)] ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via α-β-Divergence    
[[2025 CVPR](https://arxiv.org/pdf/2411.15851)] [[code](https://github.com/yvhangyang/ResCLIP)] ResCLIP: Residual Attention for Training-free Dense Vision-language Inference    
[[2025 AAAI](https://arxiv.org/pdf/2404.08181)] [[code](https://ojs.aaai.org/index.php/AAAI/article/view/32602)] Unveiling the Knowledge of CLIP for Training-Free Open-Vocabulary Semantic Segmentation     
[[2025 CVPR](https://arxiv.org/pdf/2503.19777)] [[code](https://github.com/vladan-stojnic/LPOSS/tree/main)] LOPSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation 

<a name="Graph_Structure"></a>
# Graph Structure
[[2016 AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/10302)] The Constrained Laplacian Rank Algorithm for Graph-Based Clustering    
[[2016 IJCAI](https://www.ijcai.org/Proceedings/16/Papers/269.pdf)] Parameter-Free Auto-Weighted Multiple Graph Learning: A Framework for Multiview Clustering and Semi-Supervised Classification   
[[2023 NeurIPS](https://arxiv.org/pdf/2310.05174)] [[code](https://github.com/GSL-Benchmark/GSLB)] GSLB: The Graph Structure Learning Benchmark     
[[2024 AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/28092)] [[code](https://github.com/lhf12278/FCM-ReID)] Catalyst for Clustering-based Unsupervised Object Re-Identification: Feature Calibration    
[[2025 CVPR]()] [[code]()] 
[[network](https://www.gnn.club/?p=2170)] [[code]()] 

<a name="VPR"></a>
# Visual Place Recognition
[[2022 CVPR](https://arxiv.org/pdf/2204.02287)] [[code](https://github.com/gmberton/CosPlace)] CosPlace: Rethinking Visual Geo-localization for Large-Scale Applications   
[[2024 CVPR](https://arxiv.org/pdf/2402.19231)] [[code](https://github.com/Lu-Feng/CricaVPR)] CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition    
[[2024 CVPR](https://arxiv.org/pdf/2405.07364)] [[code](https://github.com/amaralibey/Bag-of-Queries)] BoQ: A Place is Worth a Bag of Learnable Queries    
[[2024 NeurIPS](https://openreview.net/pdf?id=bZpZMdY1sj)] [[code](https://github.com/Lu-Feng/SuperVLAD)] SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition    
[[2024 ECCV](https://arxiv.org/pdf/2409.18049)] [[code](https://github.com/AnyLoc/Revisit-Anything)] Revisit Anything: Visual Place Recognition via Image Segment Retrieval      
[[2025 Arxiv](https://arxiv.org/pdf/2506.04764)] [[code](https://github.com/suhan-woo/HypeVPR)] HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition  

<a name="Token_fusion"></a>
# Token Mering, Clustering and Pruning
[[2021 NeurIPS](https://arxiv.org/pdf/2106.11297)] [[code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner)] TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?   
[[2022 CVPR](https://arxiv.org/pdf/2202.11094)] [[code](https://github.com/NVlabs/GroupViT)] GroupViT: Semantic Segmentation Emerges from Text Supervision      
[[2022 CVPR](https://arxiv.org/pdf/2308.03005)] [[code](https://github.com/xulianuwa/MCTformer)] MCTformer+: Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation    
[[2023 CVPR](https://arxiv.org/pdf/2301.12597)] [[code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models    
[[2023 ICCV](https://arxiv.org/abs/2210.09996)] Perceptual Grouping in Contrastive Vision-Language Models    
[[2023 ICLR](https://arxiv.org/pdf/2212.06795)] [[code](https://github.com/ChenhongyiYang/GPViT)] GPVIT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation    
[[2023 CVPR](https://arxiv.org/pdf/2302.12242)] [[code](https://github.com/MendelXu/SAN)] SAN: Side Adapter Network for Open-Vocabulary Semantic Segmentation    
[[2024 CVPR](https://arxiv.org/pdf/2403.10030)] [[code](https://github.com/mlvlab/MCTF)] Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers  
[[2024 CVPR](https://arxiv.org/pdf/2311.08046)] [[code](https://github.com/PKU-YuanGroup/Chat-UniVi)] Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding    
[[2024 ICLR](https://arxiv.org/pdf/2309.04669)] [[code](https://github.com/jy0205/LaVIT)] LaVIT: Unified language-vision pretraining in LLM with dynamic discrete visual tokenization    
[[2024 arXiv](https://arxiv.org/abs/2407.02392)] [[code](https://github.com/CircleRadon/TokenPacker)] TokenPacker: Efficient Visual Projector for Multimodal LLM    
[[2024 arXiv](https://arxiv.org/pdf/2405.20985)] [[code](https://github.com/yaolinli/DeCo)] DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models    
[[2024 CVPR](https://arxiv.org/pdf/2312.00878)] [[code](https://github.com/WalBouss/GEM)] Grounding Everything: Emerging Localization Properties in Vision-Language Transformers    
[[2025 CVPR](https://arxiv.org/pdf/2504.08966)] [[code](https://github.com/orailix/PACT/tree/main)] PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models   


<a name="Classical_Segmentation"></a>
# Classical segmentation method
[[2015 CVPR](https://arxiv.org/abs/1411.4038)] [[code](https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn)] FCN: Fully Convolutional Networks for Semantic Segmentation    
[[2016 MICCAI](https://arxiv.org/pdf/1505.04597)] UNet: Convolutional Networks for Biomedical Image Segmentation    
[[2017 arXiv](https://arxiv.org/pdf/1706.05587)] DeepLabV3: Rethinking atrous convolution for semantic image segmentation    
[[2018 CVPR](https://arxiv.org/pdf/1802.02611)] DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation    
[[2019 CVPR](https://arxiv.org/pdf/1901.02446)] Semantic FPN: Panoptic Feature Pyramid Networks    
[[2021 CVPR](https://arxiv.org/pdf/2012.15840)] [[code](https://github.com/fudan-zvg/SETR)] SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers    
[[2021 ICCV](https://arxiv.org/pdf/2105.05633)] [[code](https://github.com/rstrudel/segmenter)] Segmenter: Transformer for Semantic Segmentation    
[[2021 NeurIPS](https://arxiv.org/pdf/2105.15203)] [[code](https://github.com/NVlabs/SegFormer)] SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers    
[[2021 CVPR](https://arxiv.org/pdf/2107.06278)] [[code](https://github.com/facebookresearch/MaskFormer)] MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation    
[[2022 CVPR](https://arxiv.org/pdf/2112.01527)] [[code](https://github.com/facebookresearch/Mask2Former)] Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation    
[[2024 CVPR](https://arxiv.org/pdf/2312.04265)] [[code](https://github.com/w1oves/Rein)] Rein: Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation    

<a name="Classical_detection"></a>
# Classical detection method
[[2015 NeurIPS](https://arxiv.org/pdf/1506.01497)] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks    
[[2020 ECCV](https://arxiv.org/pdf/2005.12872)] [[code](https://github.com/facebookresearch/detr)] DETR: End-to-End Object Detection with Transformers    
[[2021 ICLR](https://arxiv.org/pdf/2010.04159)] [[code](https://github.com/fundamentalvision/Deformable-DETR)] Deformable DETR: Deformable Transformers for End-to-End Object Detection    
[[2023 ICLR](https://arxiv.org/pdf/2203.03605)] [[code](https://github.com/IDEA-Research/DINO)] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection    

<a name="Backbone"></a>
# Backbone
[[2017 NeurIPS](https://arxiv.org/pdf/1706.03762)] [[code](https://github.com/tensorflow/tensor2tensor)] transfomer: Attention Is All You Need    
[[2021 ICLR](https://arxiv.org/pdf/2010.11929)] [[code](https://github.com/google-research/vision_transformer)] ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale    
[[2021 ICML](https://arxiv.org/pdf/2012.12877)] DeiT: Training data-efficient image transformers & distillation through attention    
[[2021 ICCV](https://arxiv.org/pdf/2103.14030)] [[code](https://github.com/microsoft/Swin-Transformer)] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows    
[[2021 NeurIPS](https://arxiv.org/pdf/2104.13840)] [[code](https://github.com/Meituan-AutoML/Twins)] Twins: Revisiting the Design of Spatial Attention in Vision Transformers    
[[2022 CVPR](https://arxiv.org/pdf/2203.10833)] [[code](https://github.com/htdt/hyp_metric)] Hyperbolic Vision Transformers: Combining Improvements in Metric Learning      
[[2022 ICLR](https://arxiv.org/pdf/2106.08254)] [[code](https://github.com/microsoft/unilm/tree/master/beit)] BEiT: BERT Pre-Training of Image Transformers    
[[2022 CVPR](https://arxiv.org/pdf/2111.06377)] [[code](https://github.com/facebookresearch/mae)] MAE: Masked Autoencoders Are Scalable Vision Learners    
[[2022 CVPR](https://arxiv.org/pdf/2111.11418)] [[code](https://github.com/sail-sg/poolformer)] PoolFormer: MetaFormer is Actually What You Need for Vision    
[[2022 NeurIPS](https://arxiv.org/pdf/2209.08575)] [[code](https://github.com/Visual-Attention-Network/SegNeXt)] SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation    
[[2023 ICCV](https://arxiv.org/pdf/2303.08131)] [[code](https://github.com/IDEA-Research/OpenSeeD)] OpenSeeD: A simple framework for open-vocabulary segmentation and detection    
[[2023 arXiv](https://arxiv.org/pdf/2304.02643)] [[code](https://github.com/facebookresearch/segment-anything)] [[demo](https://segment-anything.com/demo)] SAM: Segment Anything    
[[2024 arXiv](https://arxiv.org/pdf/2408.00714)] [[code](https://github.com/facebookresearch/sam2)] [[demo](https://sam2.metademolab.com/)] SAM 2: Segment Anything in Images and Videos    


<a name="CLIP"></a>
# CLIP
[[2021 ICML](https://arxiv.org/pdf/2103.00020)] [[code](https://github.com/OpenAI/CLIP)] CLIP: Learning transferable visual models from natural language supervision      
[[2022 IJCV](https://arxiv.org/pdf/2109.01134)] [[code](https://github.com/KaiyangZhou/CoOp)] CoOp: Learning to Prompt for Vision-Language Models      
[[2022 ECCV](https://arxiv.org/pdf/2203.12119)] [[code](https://github.com/kmnp/vpt)] VPT: Visual Prompt Tuning      
[[2022 ICLR](https://arxiv.org/pdf/2106.09685)] [[code](https://github.com/microsoft/LoRA)] LoRA: Low-Rank Adaptation of Large Language Models     
[[2022 NeurIPS](https://arxiv.org/pdf/2209.07511)] [[code](https://github.com/azshue/TPT)] TPT: Test-Time Prompt Tuning for Zero-shot Generalization in Vision-Language Models      
[[2022 Arxiv](https://arxiv.org/pdf/2204.03649)] [[code](https://github.com/tonyhuang2022/UPL)] UPL: Unsupervised Prompt Learning for Vision-Language Models     
[[2022 Arxiv](https://arxiv.org/pdf/2212.00784)] [[code](https://github.com/jonkahana/CLIPPR)] CLIPPR: Improving Zero-Shot Models with Label Distribution Priors      
[[2022 CVPR](https://arxiv.org/pdf/2203.05557)] [[code](https://github.com/KaiyangZhou/CoOp)] CoCoOp: Conditional Prompt Learning for Vision-Language Models      
[[2023 CVPR](https://arxiv.org/pdf/2211.10277)] [[code](https://github.com/geekyutao/TaskRe)] TaskRes: Task Residual for Tuning Vision-Language Models      
[[2023 ICML](https://arxiv.org/pdf/2305.00350)] [[code](https://github.com/korawat-tanwisuth/POUF)] POUF: Prompt-Oriented Unsupervised Fine-tuning for Large Pre-trained Models      
[[2023 NeurIPS](https://arxiv.org/pdf/2306.01669)] [[code](https://github.com/BatsResearch/menghini-neurips23-code)] Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning      
[[2023 NeurIPS](https://arxiv.org/pdf/2305.18287)] [[code](https://github.com/jmiemirza/LaFTer)] LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections      
[[2023 PRCV](https://arxiv.org/pdf/2308.11507)] Unsupervised Prototype Adapter for Vision-Language Models      
[[2023 IJCV](https://arxiv.org/pdf/2110.04544)] [[code](https://github.com/gaopengcuhk/CLIP-Adapter)] CLIP-Adapter: Better Vision-Language Models with Feature Adapters      
[[2024 CVPR](https://arxiv.org/pdf/2404.17753)] [[code](https://github.com/YCaigogogo/CODER)] CODER: Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification      
[[2024 CVPR](https://arxiv.org/pdf/2404.02285)] [[code](https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline)] LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP      
[[2024 CVPR](https://arxiv.org/pdf/2403.02781)] [[code](https://github.com/zhengli97/PromptKD)] PromptKD: Unsupervised Prompt Distillation for Vision-Language Models      
[[2024 CVPR](https://arxiv.org/pdf/2405.18437)] [[code](https://github.com/SegoleneMartin/transductive-CLIP)] Transductive Zero-Shot and Few-Shot CLIP      
[[2024 CVPR](https://arxiv.org/pdf/2403.15132)] [[code](https://github.com/alwaysuu/CLIPDenoising)] Transfer CLIP for Generalizable Image Denoising      
[[2024 ECCV](https://arxiv.org/pdf/2404.07204)] BRAVE: Broadening the visual encoding of vision-language model      
[[2024 ICML](https://arxiv.org/pdf/2406.10502)] [[code](https://github.com/vanillaer/CPL-ICML2024)] Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data      
[[2024 CVPR](https://arxiv.org/pdf/2307.12732)] [[code](https://github.com/winycg/CLIP-KD)] CLIP-KD: An Empirical Study of CLIP Model Distillation    
[[2025 WACV](https://arxiv.org/pdf/2408.08855)] [[code](https://github.com/Externalhappy/DPA)] DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models      
[[2025 WACV](https://arxiv.org/pdf/2403.12952)] [[code](https://github.com/elaine-sui/TPS)] Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models      
[[2025 WACV](https://arxiv.org/pdf/2410.08211)] [[code](https://github.com/astra-vision/LatteCLIP)] LATTECLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts      
[[2025 ICLR](https://arxiv.org/pdf/2502.04263)] [[code](https://github.com/miccunifi/Cross-the-Gap)] CROSS THE GAP: EXPOSING THE INTRA-MODAL MISALIGNMENT IN CLIP VIA MODALITY INVERSION      
[[2025 ICLR](https://arxiv.org/pdf/2410.05270)] [[code](https://github.com/astra-vision/ProLIP)] CLIP’s Visual Embedding Projector is a Few-shot Cornucopia    
[[2025 CVPR](https://arxiv.org/pdf/2505.23694)]  [[code](https://github.com/Noahsark/DA-VPT)] DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers     
[[2025 ICML](https://arxiv.org/pdf/2506.02557)]  [[code](https://github.com/peterant330/KUEA)] Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models     

<a name="open_segmentation"></a>
# Open vocabulary segmentation
[[2022 ICLR](https://arxiv.org/pdf/2201.03546)] [[code](https://github.com/isl-org/lang-seg)] Lseg: Language-driven semantic segmentation (Supervised)    
[[2022 CVPR](https://arxiv.org/pdf/2112.07910)] [[code](https://github.com/dingjiansw101/ZegFormer)] ZegFormer: Decoupling Zero-Shot Semantic Segmentation    
[[2022 ECCV](https://arxiv.org/pdf/2112.01071)] [[code](https://github.com/chongzhou96/MaskCLIP)] MaskCLIP+: Extract Free Dense Labels from CLIP    
[[2022 ECCV](https://arxiv.org/pdf/2207.08455v2)] ViL-Seg: Open-World Semantic Segmentation via Contrasting and Clustering Vision-Language Embeddings    
[[2022 CVPR](https://arxiv.org/pdf/2202.11094)] [[code](https://github.com/NVlabs/GroupViT)] GroupViT: Semantic Segmentation Emerges from Text Supervision (Open-Vocabulary Zero-Shot)    
[[2022 ECCV](https://arxiv.org/pdf/2112.12143)] OpenSeg: Scaling Open-Vocabulary Image Segmentation with Image-Level Labels    
[[2023 CVPR](https://arxiv.org/pdf/2303.17225)] [[code](https://github.com/bytedance/FreeSeg)] FreeSeg: Unified, Universal, and Open-Vocabulary Image Segmentation    
[[2023 ICML](https://arxiv.org/pdf/2211.14813)] [[code](https://github.com/ArrowLuo/SegCLIP)] SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation (Zero-Shot)    
[[2023 CVPR](https://arxiv.org/pdf/2212.11270)] [[code](https://github.com/microsoft/X-Decoder/tree/main)] X-Decoder: Generalized Decoding for Pixel, Image, and Language    
[[2023 CVPR](https://arxiv.org/pdf/2303.04803)] [[code](https://github.com/NVlabs/ODISE)] ODISE: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models    
[[2023 ICML](https://arxiv.org/pdf/2208.08984)] [[code](https://github.com/mlpc-ucsd/MaskCLIP)] MaskCLIP: Open-Vocabulary Universal Image Segmentation with MaskCLIP    
[[2023 CVPR](https://arxiv.org/pdf/2302.12242)] [[code](https://github.com/MendelXu/SAN)] SAN: Side Adapter Network for Open-Vocabulary Semantic Segmentation    
[[2024 ECCV](https://arxiv.org/pdf/2312.12359)] [[code](https://github.com/wysoczanska/clip_dinoiser)] CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation    
[[2024 CVPR](https://arxiv.org/pdf/2311.15537)] [[code](https://github.com/xb534/SED)] SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation    
[[2024 TPAMI](https://arxiv.org/pdf/2306.15880)] [[code](https://github.com/jianzongwu/Awesome-Open-Vocabulary)] Review: Towards Open Vocabulary Learning: A Survey    

<a name="open_detection"></a>
# Open vocabulary object detection
[[2021 CVPR](https://arxiv.org/pdf/2011.10678)] [[code](https://github.com/alirezazareian/ovr-cnn)] Open-Vocabulary Object Detection Using Captions    
[[2022 ICLR](https://arxiv.org/pdf/2104.13921)] [[code](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)] ViLD: Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation    
[[2022 CVPR](https://arxiv.org/pdf/2112.03857)] [[code](https://github.com/microsoft/GLIP)] GLIP: Grounded Language-Image Pre-training    
[[2022 NeurIPS](https://arxiv.org/pdf/2206.05836)] [[code](https://github.com/microsoft/GLIP)] GLIPv2: Unifying Localization and Vision-Language Understanding    

<a name="Other"></a>
# Other Technologies
[[2016 CVPRW](https://arxiv.org/pdf/1609.05158)] pixel shuffle    
[[2020 NeurIPS](https://arxiv.org/pdf/2006.11239)] [[code](https://github.com/hojonathanho/diffusion)] DDPM: Denoising Diffusion Probabilistic Models    
